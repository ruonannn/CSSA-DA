"""
æ¨¡å—äºŒï¼šRetriever - ç®€åŒ–ç‰ˆè¯­ä¹‰æ£€ç´¢æ¨¡å—
ä½¿ç”¨åŸºç¡€çš„scikit-learnå’Œnumpyå®ç°

ä½œè€…ï¼šruonan (xrn zmy syl slyå›¢é˜Ÿ)
åˆ›å»ºæ—¶é—´ï¼š2025-08-18
"""

import json
import numpy as np
from typing import List, Dict, Tuple
import os
import pickle
from dataclasses import dataclass
import logging
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
import re

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç±»"""
    question_id: str
    question: str
    answer: str
    source: str
    link: str
    tags: List[str]
    similarity_score: float

class SimpleRetriever:
    """
    ç®€åŒ–ç‰ˆè¯­ä¹‰æ£€ç´¢å™¨ï¼ˆä½¿ç”¨TF-IDF + ä½™å¼¦ç›¸ä¼¼åº¦ï¼‰
    
    åŠŸèƒ½ï¼š
    - åŠ è½½é—®ç­”æ•°æ®
    - ä½¿ç”¨TF-IDFè¿›è¡Œæ–‡æœ¬å‘é‡åŒ–
    - æ‰§è¡ŒåŸºäºä½™å¼¦ç›¸ä¼¼åº¦çš„æ£€ç´¢
    """
    
    def __init__(
        self, 
        data_path: str = "data/qa_clean_data.json",
        vectorizer_save_path: str = "retriever/tfidf_vectorizer.pkl",
        vectors_save_path: str = "retriever/question_vectors.pkl",
        id_mapping_save_path: str = "retriever/id_mapping.json"
    ):
        """åˆå§‹åŒ–æ£€ç´¢å™¨"""
        self.data_path = data_path
        self.vectorizer_save_path = vectorizer_save_path
        self.vectors_save_path = vectors_save_path
        self.id_mapping_save_path = id_mapping_save_path
        
        self.qa_data = []
        self.vectorizer = None
        self.question_vectors = None
        self.id_to_data_mapping = {}
        
        logger.info(f"SimpleRetrieveråˆå§‹åŒ–å®Œæˆ")
    
    def preprocess_text(self, text: str) -> str:
        """æ–‡æœ¬é¢„å¤„ç†"""
        if not text:
            return ""
        
        # å»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', ' ', text)
        
        # ä½¿ç”¨jiebaåˆ†è¯
        words = jieba.cut(text.strip())
        
        # è¿‡æ»¤åœç”¨è¯å’Œé•¿åº¦å°äº2çš„è¯
        filtered_words = [word for word in words if len(word.strip()) >= 2]
        
        return ' '.join(filtered_words)
    
    def load_data(self) -> bool:
        """åŠ è½½é—®ç­”æ•°æ®"""
        try:
            with open(self.data_path, 'r', encoding='utf-8') as f:
                self.qa_data = json.load(f)
            
            # åˆ›å»ºIDåˆ°æ•°æ®çš„æ˜ å°„
            for idx, item in enumerate(self.qa_data):
                self.id_to_data_mapping[idx] = item
            
            logger.info(f"æˆåŠŸåŠ è½½ {len(self.qa_data)} æ¡é—®ç­”æ•°æ®")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½æ•°æ®å¤±è´¥ï¼š{e}")
            return False
    
    def build_vectors(self):
        """æ„å»ºé—®é¢˜å‘é‡"""
        # é¢„å¤„ç†æ‰€æœ‰é—®é¢˜
        questions = [self.preprocess_text(item['question']) for item in self.qa_data]
        
        logger.info(f"å¼€å§‹æ„å»ºTF-IDFå‘é‡ï¼Œé—®é¢˜æ•°é‡ï¼š{len(questions)}")
        
        # åˆ›å»ºTF-IDFå‘é‡åŒ–å™¨
        self.vectorizer = TfidfVectorizer(
            max_features=5000,  # æœ€å¤§ç‰¹å¾æ•°
            ngram_range=(1, 2),  # ä½¿ç”¨1-gramå’Œ2-gram
            min_df=1,  # æœ€å°æ–‡æ¡£é¢‘ç‡
            max_df=0.8  # æœ€å¤§æ–‡æ¡£é¢‘ç‡
        )
        
        # æ‹Ÿåˆå¹¶è½¬æ¢é—®é¢˜
        self.question_vectors = self.vectorizer.fit_transform(questions)
        
        logger.info(f"TF-IDFå‘é‡æ„å»ºå®Œæˆï¼Œç‰¹å¾ç»´åº¦ï¼š{self.question_vectors.shape}")
    
    def save_components(self):
        """ä¿å­˜å‘é‡åŒ–å™¨å’Œå‘é‡"""
        try:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(self.vectorizer_save_path), exist_ok=True)
            
            # ä¿å­˜TF-IDFå‘é‡åŒ–å™¨
            with open(self.vectorizer_save_path, 'wb') as f:
                pickle.dump(self.vectorizer, f)
            
            # ä¿å­˜é—®é¢˜å‘é‡
            with open(self.vectors_save_path, 'wb') as f:
                pickle.dump(self.question_vectors, f)
            
            # ä¿å­˜IDæ˜ å°„
            with open(self.id_mapping_save_path, 'w', encoding='utf-8') as f:
                json.dump(self.id_to_data_mapping, f, ensure_ascii=False, indent=2)
            
            logger.info("ç»„ä»¶ä¿å­˜å®Œæˆ")
            
        except Exception as e:
            logger.error(f"ä¿å­˜å¤±è´¥ï¼š{e}")
            raise e
    
    def load_components(self) -> bool:
        """åŠ è½½å·²ä¿å­˜çš„ç»„ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not all(os.path.exists(path) for path in [
                self.vectorizer_save_path, 
                self.vectors_save_path, 
                self.id_mapping_save_path
            ]):
                logger.warning("ç»„ä»¶æ–‡ä»¶ä¸å®Œæ•´ï¼Œéœ€è¦é‡æ–°æ„å»º")
                return False
            
            # åŠ è½½TF-IDFå‘é‡åŒ–å™¨
            with open(self.vectorizer_save_path, 'rb') as f:
                self.vectorizer = pickle.load(f)
            
            # åŠ è½½é—®é¢˜å‘é‡
            with open(self.vectors_save_path, 'rb') as f:
                self.question_vectors = pickle.load(f)
            
            # åŠ è½½IDæ˜ å°„
            with open(self.id_mapping_save_path, 'r', encoding='utf-8') as f:
                loaded_mapping = json.load(f)
                self.id_to_data_mapping = {int(k): v for k, v in loaded_mapping.items()}
            
            logger.info("ç»„ä»¶åŠ è½½å®Œæˆ")
            return True
            
        except Exception as e:
            logger.error(f"åŠ è½½å¤±è´¥ï¼š{e}")
            return False
    
    def search(self, query: str, k: int = 5) -> List[RetrievalResult]:
        """æ‰§è¡Œè¯­ä¹‰æ£€ç´¢"""
        try:
            # é¢„å¤„ç†æŸ¥è¯¢
            processed_query = self.preprocess_text(query)
            
            # å°†æŸ¥è¯¢è½¬æ¢ä¸ºå‘é‡
            query_vector = self.vectorizer.transform([processed_query])
            
            # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
            similarities = cosine_similarity(query_vector, self.question_vectors).flatten()
            
            # è·å–top-kç»“æœçš„ç´¢å¼•
            top_indices = np.argsort(similarities)[::-1][:k]
            
            # æ„å»ºç»“æœ
            results = []
            for idx in top_indices:
                if similarities[idx] > 0:  # åªè¿”å›æœ‰ç›¸ä¼¼åº¦çš„ç»“æœ
                    data_item = self.id_to_data_mapping[idx]
                    result = RetrievalResult(
                        question_id=data_item['id'],
                        question=data_item['question'],
                        answer=data_item['answer'],
                        source=data_item.get('source', ''),
                        link=data_item.get('link', ''),
                        tags=data_item.get('tags', []),
                        similarity_score=float(similarities[idx])
                    )
                    results.append(result)
            
            logger.info(f"æ£€ç´¢å®Œæˆï¼Œè¿”å› {len(results)} ä¸ªç»“æœ")
            return results
            
        except Exception as e:
            logger.error(f"æ£€ç´¢å¤±è´¥ï¼š{e}")
            return []
    
    def build(self, force_rebuild: bool = False):
        """æ„å»ºå®Œæ•´çš„æ£€ç´¢ç³»ç»Ÿ"""
        logger.info("å¼€å§‹æ„å»ºSimpleRetrieverç³»ç»Ÿ...")
        
        # 1. åŠ è½½æ•°æ®
        if not self.load_data():
            raise Exception("æ•°æ®åŠ è½½å¤±è´¥")
        
        # 2. å°è¯•åŠ è½½å·²æœ‰çš„ç»„ä»¶
        if not force_rebuild and self.load_components():
            logger.info("ä½¿ç”¨å·²æœ‰ç»„ä»¶")
            return
        
        # 3. æ„å»ºå‘é‡
        self.build_vectors()
        
        # 4. ä¿å­˜ç»„ä»¶
        self.save_components()
        
        logger.info("SimpleRetrieverç³»ç»Ÿæ„å»ºå®Œæˆï¼")


def test_retriever():
    """æµ‹è¯•æ£€ç´¢å™¨åŠŸèƒ½"""
    # åˆå§‹åŒ–æ£€ç´¢å™¨
    retriever = SimpleRetriever()
    
    # æ„å»ºæ£€ç´¢ç³»ç»Ÿ
    retriever.build()
    
    # æµ‹è¯•æ£€ç´¢
    test_queries = [
        "å¢¨å°”æœ¬æ€ä¹ˆåå…¬äº¤è½¦ï¼Ÿ",
        "å¦‚ä½•ä½¿ç”¨Mykiå¡ï¼Ÿ",
        "å­¦ç”Ÿä¹˜è½¦æœ‰ä¼˜æƒ å—ï¼Ÿ",
        "å¢¨å°”æœ¬çš„äº¤é€šå·¥å…·æœ‰å“ªäº›ï¼Ÿ",
        "å…¬å…±äº¤é€š",
        "ç§Ÿæˆ¿"
    ]
    
    print("\n" + "="*60)
    print("ğŸš€ å¼€å§‹æµ‹è¯•ç®€åŒ–ç‰ˆè¯­ä¹‰æ£€ç´¢åŠŸèƒ½")
    print("="*60)
    
    for query in test_queries:
        print(f"\nğŸ” æŸ¥è¯¢ï¼š{query}")
        print("-" * 40)
        
        results = retriever.search(query, k=3)
        
        if not results:
            print("   æ²¡æœ‰æ‰¾åˆ°ç›¸å…³ç»“æœ")
            continue
            
        for i, result in enumerate(results, 1):
            print(f"{i}. ğŸ“Š ç›¸ä¼¼åº¦: {result.similarity_score:.3f}")
            print(f"   â“ é—®é¢˜: {result.question}")
            print(f"   âœ… ç­”æ¡ˆ: {result.answer}")
            print(f"   ğŸ·ï¸  æ ‡ç­¾: {result.tags}")
            if result.source:
                print(f"   ğŸ“š æ¥æº: {result.source}")
            print()


if __name__ == "__main__":
    test_retriever()
