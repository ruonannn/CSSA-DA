{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        question                                         answer\n",
      "0      å¦‚ä½•åŠ å…¥CSSAï¼Ÿ              ä½ å¯ä»¥å…³æ³¨CSSAçš„å¾®ä¿¡å…¬ä¼—å·ï¼Œæˆ–è€…åœ¨ç¤¾äº¤å¹³å°ä¸Šè”ç³»å·¥ä½œäººå‘˜æŠ¥åã€‚\n",
      "1  å¢¨å°”æœ¬å¤§å­¦æœ‰å“ªäº›é€‰è¯¾å»ºè®®ï¼Ÿ                  å»ºè®®é€‰æ‹©è‡ªå·±æ„Ÿå…´è¶£çš„è¯¾ç¨‹ï¼ŒåŒæ—¶æŸ¥çœ‹è¯¾ç¨‹è¯„åˆ†å’Œå¾€å¹´è€ƒè¯•éš¾åº¦ã€‚\n",
      "2   CSSAä¼šä¸¾åŠå“ªäº›æ´»åŠ¨ï¼Ÿ                  CSSAå®šæœŸä¸¾åŠè¿æ–°ä¼šã€è”è°Šæ´»åŠ¨ã€å­¦æœ¯è®²åº§ã€æ±‚èŒåˆ†äº«ä¼šç­‰ã€‚\n",
      "3       å¢¨å°”æœ¬å¦‚ä½•ç§Ÿæˆ¿ï¼Ÿ  å¯ä»¥é€šè¿‡Facebookç¾¤ç»„ã€Flatmatesã€CSSAç§Ÿæˆ¿ä¿¡æ¯ç¾¤ç­‰æ¸ é“å¯»æ‰¾åˆé€‚çš„æˆ¿æºã€‚\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# è¯»å– CSV æ–‡ä»¶\n",
    "df = pd.read_csv(\"cssa_data.csv\")\n",
    "\n",
    "# æ˜¾ç¤ºå‰å‡ æ¡æ•°æ®\n",
    "print(df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda3\\envs\\CSSA\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['question', 'answer'],\n",
      "    num_rows: 4\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "# åŠ è½½ CSV æ•°æ®é›†\n",
    "dataset = Dataset.from_pandas(df)\n",
    "\n",
    "# æŸ¥çœ‹æ•°æ®æ ¼å¼\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… FAISS å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼ç´¢å¼•å¤§å°: 4\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import gc\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# âœ… 1ï¸âƒ£ è¯»å–å°æ•°æ®é›†\n",
    "file_path = os.path.abspath(\"cssa_data.csv\")\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# âœ… 2ï¸âƒ£ åŠ è½½ Transformer æ¨¡å‹ï¼ˆæ›´è½»é‡çº§ï¼‰\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "model = AutoModel.from_pretrained(\"sentence-transformers/paraphrase-MiniLM-L6-v2\")\n",
    "\n",
    "# è®¾ç½®è®¾å¤‡ï¼ˆç¡®ä¿åªä½¿ç”¨ CPUï¼Œé¿å… GPU ç›¸å…³é—®é¢˜ï¼‰\n",
    "device = torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# âœ… 3ï¸âƒ£ è®¡ç®—æ–‡æœ¬åµŒå…¥ï¼ˆä»… 4 æ¡æ•°æ®ï¼Œæ— éœ€æ‰¹é‡å¤„ç†ï¼‰\n",
    "def embed_text(text):\n",
    "    if not isinstance(text, str) or len(text.strip()) == 0:\n",
    "        return np.zeros((384,), dtype=np.float32)  # è¿”å›å›ºå®šç»´åº¦çš„é›¶å‘é‡\n",
    "\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=32).to(device)\n",
    "\n",
    "    with torch.no_grad():  \n",
    "        output = model(**inputs)\n",
    "\n",
    "    return output.last_hidden_state.mean(dim=1).cpu().numpy().flatten().astype(np.float32)  # ä¿è¯æ˜¯ (384,)\n",
    "\n",
    "# âœ… 4ï¸âƒ£ åˆ›å»º FAISS ç´¢å¼•\n",
    "dimension = 384\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# âœ… 5ï¸âƒ£ è®¡ç®— 4 æ¡æ•°æ®çš„å‘é‡ï¼Œå¹¶å­˜å…¥ FAISS\n",
    "question_vectors = np.array([embed_text(q) for q in df[\"question\"]], dtype=np.float32)\n",
    "index.add(question_vectors)  \n",
    "\n",
    "print(f\"âœ… FAISS å‘é‡ç´¢å¼•åˆ›å»ºå®Œæˆï¼ç´¢å¼•å¤§å°: {index.ntotal}\")\n",
    "\n",
    "# âœ… 6ï¸âƒ£ é‡Šæ”¾ Python å†…å­˜ï¼Œé˜²æ­¢ Jupyter å†…æ ¸å´©æºƒ\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# âœ… 7ï¸âƒ£ å®šä¹‰ä¸€ä¸ªæ£€ç´¢å‡½æ•°\n",
    "def retrieve_answer(user_query):\n",
    "    if index.ntotal == 0:\n",
    "        return \"âŒ é”™è¯¯: FAISS ç´¢å¼•ä¸ºç©ºï¼Œæ— æ³•æŸ¥è¯¢ç­”æ¡ˆï¼\"\n",
    "\n",
    "    query_vector = embed_text(user_query).reshape(1, -1).astype(np.float32)  # è½¬æ¢ float32\n",
    "    distances, indices = index.search(query_vector, k=1)  \n",
    "\n",
    "    if indices[0][0] == -1:\n",
    "        return \"âŒ é”™è¯¯: æ‰¾ä¸åˆ°åŒ¹é…çš„ç­”æ¡ˆï¼\"\n",
    "\n",
    "    best_match = df.iloc[indices[0][0]]  \n",
    "    return best_match[\"answer\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” æŸ¥è¯¢: CSSAæ˜¯ä»€ä¹ˆ\n",
      "ğŸ“Œ æœ€ä½³åŒ¹é…ç­”æ¡ˆ: ä½ å¯ä»¥å…³æ³¨CSSAçš„å¾®ä¿¡å…¬ä¼—å·ï¼Œæˆ–è€…åœ¨ç¤¾äº¤å¹³å°ä¸Šè”ç³»å·¥ä½œäººå‘˜æŠ¥åã€‚\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# âœ… 8ï¸âƒ£ æµ‹è¯•æŸ¥è¯¢\n",
    "query = \"CSSAæ˜¯ä»€ä¹ˆ\"\n",
    "response = retrieve_answer(query)\n",
    "print(f\"ğŸ” æŸ¥è¯¢: {query}\\nğŸ“Œ æœ€ä½³åŒ¹é…ç­”æ¡ˆ: {response}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
